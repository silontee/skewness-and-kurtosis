# 3. Data Description

## 3.1. Match Outcomes: International Football Results Dataset
In sports analytics, modeling the aggregate scoring intensity is a fundamental task for strength estimation and match forecasting. The international football dataset provides historical records and match features for 49,071 international matches spanning from 1872 to 2026. The raw dataset includes several features such as **match date, home and away team identities, tournament type, and venue locations (city and country)**. For this study, we focus on the aggregate goal count per match. 

This dataset is a prototypical candidate for orthogonal expansions because it exhibits significant over-dispersion—with a sample mean of 2.94 goals and a notably larger variance of 4.38—and a pronounced right tail where rare, high-scoring events reach a maximum of 31 goals. Approximately 67.3% of the matches result in 3 goals or fewer, creating a distribution with a high concentration of low-count outcomes but significant volatility in the upper percentiles. Using a Negative Binomial (NB) expansion, a researcher can utilize Meixner polynomials to adjust for the skewness and kurtosis caused by these extreme scoring events, "graduating" the frequency curve to account for the heavy right tail.

## 3.2. Medical Cost Discretization: Health Insurance Dataset
In the field of actuarial science, modeling the distribution of insurance claims is crucial for risk premium calculation. The "Medical Cost Personal Dataset" comprises 1,338 individual records featuring demographic attributes—such as **age, sex, BMI, number of children, and smoking status**—and total medical expenditures (**charges**). Originally featured in *Machine Learning with R* by Brett Lantz, this public domain dataset has been refined and recoded for consistency and accessibility.

While the original charges are continuous, we apply a discretization (binning) process to transform them into pseudo-count data suitable for orthogonal expansions. By setting a bin width of $3,000 and applying a 95th percentile Winsorization (capping) to mitigate the influence of extreme outliers, the dataset is mapped onto a discrete support. This transformed dataset exhibits a distinct bi-modal distribution—primarily driven by the discrepancy between smokers and non-smokers—and a high variance-to-mean ($V/M$) ratio, often exceeding 5.0. Such characteristics pose a significant challenge for traditional Poisson expansions, necessitating Meixner-based models to capture the secondary peak.

## 3.3. Synthetic Benchmarks: Bimodal Stress Test and Convergence Analysis
To rigorously evaluate the proposed SNP expansions under controlled environments, we designed two distinct synthetic datasets. These simulations mimic populations composed of distinct subgroups to test the model's robustness and theoretical validity.

### 3.3.1. Scenario A: High-Variance Stress Test (Simul_Heavy)
Scenario A is designed as a stress test to evaluate model stability under extreme over-dispersion. The data is generated by mixing two Negative Binomial (NB) distributions with widely separated means:
* **Component 1 (40%):** $n=15, \mu \approx 20$
* **Component 2 (60%):** $n=10, \mu \approx 80$

The resulting synthetic population ($n=4,000$) is characterized by an extreme $V/M$ ratio ($V/M > 20$) and a sharp valley between two distant peaks. This design intentionally violates the unimodal assumption of standard parametric distributions and is used to compare the performance of Meixner-based expansions against traditional baselines in high-volatility environments.

### 3.3.2. Scenario B: Low-Variance Convergence Benchmark (Simul_Success)
Scenario B serves as a theoretical benchmark to demonstrate the systematic convergence of the expansion as the polynomial order increases. This "Clean Bimodal" dataset is generated using a mixture of two Poisson distributions:
* **Component 1 (50%):** $\mu = 3$
* **Component 2 (50%):** $\mu = 8$

With a relatively low $V/M$ ratio (approx. 1.8) and 10,000 samples, this scenario provides a numerically stable environment. It is primarily utilized to observe the step-by-step reduction in L1 discrepancy as the expansion order increases from $K=0$ to $K=8$, providing empirical evidence for the model's shape recovery capabilities.